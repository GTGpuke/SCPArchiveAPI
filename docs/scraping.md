# üï∑Ô∏è Strat√©gie de Scraping d√©taill√©e

## üéØ Objectif
Synchroniser la base locale MongoDB avec le contenu du wiki SCP officiel, tout en respectant les ressources du site source et en assurant la qualit√© des donn√©es.

---

## üîÑ Cycle de vie du scraping
1. **Scraping initial**
   - Extraction de tous les articles SCP (par lot, avec d√©lai entre requ√™tes)
   - Indexation des articles, m√©tadonn√©es, tags
   - Stockage dans MongoDB (upsert)
2. **Mises √† jour r√©guli√®res**
   - Scraping diff√©rentiel bas√© sur la date de derni√®re modification
   - V√©rification quotidienne (job Quartz √† 03:00 UTC)
   - Mise √† jour des articles modifi√©s, ajout des nouveaux
3. **Scraping manuel**
   - D√©clench√© via l'API (endpoint POST)
   - Utilis√© pour forcer la mise √† jour d'un article sp√©cifique

---

## ‚öôÔ∏è Configuration

| Cl√©                  | Description                                 | Exemple                                  |
|----------------------|---------------------------------------------|------------------------------------------|
| `BaseUrl`            | URL du wiki SCP                             | `http://www.scp-wiki.net`                |
| `DelayBetweenRequests`| D√©lai (ms) entre chaque requ√™te             | `1000`                                   |
| `UserAgent`          | User-Agent personnalis√©                     | `SCPArchive-Bot/1.0 (contact@...)`       |
| `Timeout`            | Timeout HTTP (ms)                           | `10000`                                  |

**Exemple (appsettings.json) :**
```json
"Scraping": {
  "BaseUrl": "http://www.scp-wiki.net",
  "DelayBetweenRequests": 1000,
  "UserAgent": "SCPArchive-Bot/1.0 (contact@example.com)"
}
```

---

## üß© Extraction des donn√©es
- **HtmlAgilityPack** : Parsing du HTML, extraction des sections (titre, classe, description, addenda, tags)
- **Pattern d'extraction** :
  - XPath/CSS selectors pour cibler les blocs de contenu
  - Regex pour extraire la classe d'objet, la note, etc.
- **Gestion des addenda** : Extraction des blocs additionnels (Addendum, Notes, etc.)

---

## üö® Gestion des erreurs
- Retry avec backoff exponentiel en cas d'√©chec
- Logging d√©taill√© (succ√®s, erreurs, temps de scraping)
- Alertes si trop d'√©checs cons√©cutifs

---

## ‚è∞ Planification & orchestration
- **Quartz.NET** : Planification des jobs de scraping (full, delta, manuel)
- **Docker Compose** : Orchestration multi-service (API, MongoDB, Prometheus)
- **Logs** : Historique des jobs, dur√©e, nombre d'articles scrap√©s

---

## ü§ù Respect du site source
- Respect du `robots.txt`
- D√©lai configurable entre les requ√™tes
- User-Agent explicite
- Pas de scraping massif en journ√©e (UTC)

---

## ‚úÖ Bonnes pratiques
- Sauvegarde r√©guli√®re de la base MongoDB
- Monitoring du taux de succ√®s/√©chec
- Limitation du nombre de requ√™tes simultan√©es
- Possibilit√© de d√©sactiver le scraping via variable d'environnement

---

## üìù Exemple de log de scraping
```
[2025-06-16 03:00:00] D√©marrage du scraping diff√©rentiel
[2025-06-16 03:00:01] Article SCP-173 mis √† jour
[2025-06-16 03:00:02] Article SCP-682 inchang√©
[2025-06-16 03:00:10] Scraping termin√© : 1 article mis √† jour, 0 erreur
```

---

## üìö Pour aller plus loin
- [Sch√©ma de la base](database-schema.md)
- [Architecture](architecture.md)
- [Monitoring & m√©triques](prometheus.md)
- [D√©ploiement](deployment.md)
